{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important note:\n",
    "# I haven't taken any inspiration from Professor Rayid's magic loop for this assignment. \n",
    "# I didn't even know his magic loop existed uptil a couple of days ago, so I have written every thing based on\n",
    "# the instructions entirely myself and a little bit of useful code inspirations from stackoverflow.com\n",
    "\n",
    "# Not having Professor Rayid's code also helped me think how to construct a custom 'magic loop' without having\n",
    "# looked at one myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing the functions/variables from PA3_PreProcess_and_MakeFeatures (code inspiration: stackoverflow.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, os, sys, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_notebook(fullname='PA3_PreProcess_and_MakeFeatures', path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "    \n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "        \n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "        \n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PA3_PreProcess_and_MakeFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building various classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first split the data into the training set and the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_important_features = PA3_PreProcess_and_MakeFeatures.df_important_features\n",
    "df = PA3_PreProcess_and_MakeFeatures.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user-defined test_size_ratio\n",
    "# will be good to do cross-validation but given the processing time for this assignment, I thought it will be wise to not touch this parameter\n",
    "def train_test_split_ratio(important_features, outcome_variable, test_size_ratio):\n",
    "    return train_test_split(important_features, outcome_variable, test_size=test_size_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delinquincy_train, delinquincy_test, outcomes_train, outcomes_test = train_test_split_ratio(df_important_features, df.SeriousDlqin2yrs, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Decision_Tree = DecisionTreeClassifier()\n",
    "Logistic_Regression = LogisticRegression()\n",
    "Support_Vector_Machine = svm.LinearSVC()\n",
    "K_NearestNeighbors = KNeighborsClassifier()\n",
    "Random_Forest_Classifier = RandomForestClassifier()\n",
    "Gradient_Boosting_Classifier = GradientBoostingClassifier()\n",
    "Bagging_Classifier = BaggingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [Decision_Tree, Logistic_Regression, Support_Vector_Machine, K_NearestNeighbors, Random_Forest_Classifier, Gradient_Boosting_Classifier, Bagging_Classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_names = ['Decision_Tree', 'Logistic_Regression', 'Support_Vector_Machine', 'K_NearestNeighbors', 'Random_Forest_Classifier', 'Gradient_Boosting_Classifier', 'Bagging_Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifier_fit(classifier, X_train, Y_train, X_output, Y_output):\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_output, Y_output)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_classifiers(classifiers_names, X_train, Y_train, X_output, Y_output):\n",
    "    score_classifiers_list = []\n",
    "    for classifier in classifiers_names:\n",
    "        score = classifier_fit(classifier, X_train, Y_train, X_output, Y_output)\n",
    "        score_classifiers_list.append((classifier, score))\n",
    "    return score_classifiers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_classifiers_list = model_classifiers(classifiers, delinquincy_train, outcomes_train, delinquincy_test, outcomes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              presort=False, random_state=None, splitter='best'),\n",
       "  0.89817777777777774),\n",
       " (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False), 0.9349777777777778),\n",
       " (LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "       verbose=0), 0.93535555555555561),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 0.93031111111111109),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "              verbose=0, warm_start=False), 0.93324444444444443),\n",
       " (GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                n_estimators=100, presort='auto', random_state=None,\n",
       "                subsample=1.0, verbose=0, warm_start=False),\n",
       "  0.93662222222222224),\n",
       " (BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "           bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False), 0.93224444444444443)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_classifiers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Printing out the classifiers and scores for their default parameters in a dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def default_estimators_score(score_classifiers_list_1):\n",
    "    accuracy_score = []\n",
    "\n",
    "    for classifier in score_classifiers_list_1:\n",
    "        accuracy_score.append(classifier[1])\n",
    "                \n",
    "    results_df = pd.DataFrame({'classifiers': classifiers_names, 'default_accuracy_score': accuracy_score})\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators_default_score_df = default_estimators_score(score_classifiers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifiers</th>\n",
       "      <th>default_accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision_Tree</td>\n",
       "      <td>0.898178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K_NearestNeighbors</td>\n",
       "      <td>0.930311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bagging_Classifier</td>\n",
       "      <td>0.932244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random_Forest_Classifier</td>\n",
       "      <td>0.933244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>0.934978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support_Vector_Machine</td>\n",
       "      <td>0.935356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient_Boosting_Classifier</td>\n",
       "      <td>0.936622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    classifiers  default_accuracy_score\n",
       "0                 Decision_Tree                0.898178\n",
       "3            K_NearestNeighbors                0.930311\n",
       "6            Bagging_Classifier                0.932244\n",
       "4      Random_Forest_Classifier                0.933244\n",
       "1           Logistic_Regression                0.934978\n",
       "2        Support_Vector_Machine                0.935356\n",
       "5  Gradient_Boosting_Classifier                0.936622"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators_default_score_df.sort_values(by='default_accuracy_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows that out of all the classifiers, the Decision Tree performs the worst, and the Gradient Boosting Classifier performs the best with the accuracy rate of 0.93 or 93%. Accuracy is defined as the percentage of test cases that were accurately classified as either being delinquint (class 1) or non-delinquint (class 0).\n",
    "\n",
    "The Gradient Boosting Classifier uses an ensemble of decision trees with different parameters. It performs ensemble methods in a forward stage-wise fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweaking the parameters to see changes in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_dt = {\n",
    "    Decision_Tree: {'max_features': ['auto', 'sqrt', 'log2'], 'class_weight':['balanced', None]}, \n",
    "    K_NearestNeighbors: {'n_neighbors': list(range(1,11)), 'p':[1,2]},\n",
    "    Logistic_Regression: {'penalty': ['l1', 'l2'], 'C':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "    Support_Vector_Machine: {'class_weight':['balanced', None], 'C':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "    Random_Forest_Classifier : {'max_features':['auto', 'sqrt', 'log2'], 'criterion': ['gini', 'entropy']},\n",
    "    Gradient_Boosting_Classifier: {'loss': ['deviance', 'exponential'], 'criterion': ['friedman_mse', 'mse', 'mae']},\n",
    "    Bagging_Classifier : {'n_estimators': list(range(11)), 'bootstrap_features': ['True', 'False']}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parameter_tuning(classifiers_1):\n",
    "    \n",
    "    d_best_estimators = {}\n",
    "    \n",
    "    for classifier in classifiers_1:\n",
    "        \n",
    "        grid = GridSearchCV(estimator=classifier, param_grid=param_grid_dt[classifier])\n",
    "        grid.fit(delinquincy_train, outcomes_train)\n",
    "        \n",
    "        best_params = grid.best_params_\n",
    "        d_best_estimators[classifier] = best_params\n",
    "        accuracy_best_score = grid.best_score_\n",
    "        d_best_estimators[classifier]['best_accuracy_score'] = accuracy_best_score\n",
    "            \n",
    "    return d_best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimators = parameter_tuning(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we will print out the classifiers, the names of their best dual features, and scores for their default parameters in a dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_estimators_lists(best_estimators_1):\n",
    "    classifier_list = []\n",
    "    best_accuracy_score = []\n",
    "    feature_one_name = []\n",
    "    feature_one_value = []\n",
    "    feature_two_name = []\n",
    "    feature_two_value = []\n",
    "\n",
    "    for classifier in best_estimators_1:\n",
    "        classifier_list.append(classifier)\n",
    "        \n",
    "        counter = 0\n",
    "        for feature in best_estimators_1[classifier]:\n",
    "            if feature == 'best_accuracy_score':\n",
    "                best_accuracy_score.append(best_estimators_1[classifier][feature])\n",
    "                \n",
    "            elif feature != 'best_accuracy_score' and counter == 0:\n",
    "                feature_one_name.append(feature)\n",
    "                feature_one_value.append(best_estimators_1[classifier][feature])\n",
    "                counter += 1\n",
    "                \n",
    "            elif feature != 'best_accuracy_score' and counter == 1:\n",
    "                feature_two_name.append(feature)\n",
    "                feature_two_value.append(best_estimators_1[classifier][feature])\n",
    "                counter += 1\n",
    "                \n",
    "    results_df = pd.DataFrame({'classifiers':classifier_list, 'feature_one_name':feature_one_name, 'feature_one_value':feature_one_value, 'feature_two_name':feature_two_name, 'feature_two_value':feature_two_value})\n",
    "    results_df['accuracy_score'] = best_accuracy_score\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimators_scores = best_estimators_lists(best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimators_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting all the classifiers to their best estimated features/values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for classifier in best_estimators:\n",
    "    for param in classifier.get_params():\n",
    "        if param in best_estimators[classifier]:\n",
    "            setattr (classifier, param, best_estimators[classifier][param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating the fit of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluating on the classifiers with their best parameters i.e. parameters that give the highest accuracy score\n",
    "\n",
    "def classifiers_evaluation(X_test, Y_test):\n",
    "    class_zero_precision = []\n",
    "    class_zero_recall = []\n",
    "    class_zero_fscore = []\n",
    "    class_one_precision = []\n",
    "    class_one_recall = []\n",
    "    class_one_fscore = []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "        predicted = classifier.predict(X_test)\n",
    "        precision, recall, thresholds = precision_recall_curve(Y_test, predicted)\n",
    "        precision_result, recall_result, fscore_result, support_result = precision_recall_fscore_support(Y_test, predicted)\n",
    "        #print(classification_report(Y_test, predicted))\n",
    "        \n",
    "        for i in range(len(precision_result)):\n",
    "            if i == 0:  \n",
    "                class_zero_precision.append(precision_result[i])\n",
    "                class_zero_recall.append(recall_result[i])\n",
    "                class_zero_fscore.append(fscore_result[i])\n",
    "                \n",
    "            elif i == 1:\n",
    "                class_one_precision.append(precision_result[i])\n",
    "                class_one_recall.append(recall_result[i])\n",
    "                class_one_fscore.append(fscore_result[i])\n",
    "        \n",
    "        plt.title('Precision Recall Curve')\n",
    "        plt.title(classifiers_names[i] + ' ' + 'Precision Recall Curve')\n",
    "        plt.plot(precision, recall)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0,1],[0,1],'r--')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "        plt.ylim([-0.1,1.2])\n",
    "        plt.ylabel('Recall')\n",
    "        plt.xlabel('Precision')\n",
    "        plt.show()\n",
    "        i += 1\n",
    "\n",
    "    return classifiers_evaluation_list, class_zero_precision, class_zero_recall, class_zero_fscore, class_one_precision, class_one_recall, class_one_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifiers_evaluation_list_names, class_zero_precision, class_zero_recall, class_zero_fscore, class_one_precision, class_one_recall, class_one_fscore = classifiers_evaluation(delinquincy_test, outcomes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df_class_zero = pd.DataFrame({'Class': [0]*len(classifiers_evaluation_list_names), 'classifiers':classifiers, 'precision':class_zero_precision, 'recall':class_zero_recall, 'fscore':class_zero_fscore})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df_class_one = pd.DataFrame({'Class': [1]*len(classifiers_evaluation_list_names), 'classifiers':classifiers, 'precision':class_one_precision, 'recall':class_one_recall, 'fscore':class_one_fscore})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df_class_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df_class_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = results_df_class_zero.append(results_df_class_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating roc and auc curves/scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from ggplot import *\n",
    "\n",
    "def calculate_roc_and_auc_2(X_test, y_test):\n",
    "    \n",
    "    roc_auc_score = []\n",
    "    i = 0\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        \n",
    "        probs = classifier.predict_proba(X_test)\n",
    "        preds = probs[:,1]\n",
    "        fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc_score.append(roc_auc)\n",
    "\n",
    "        # method I: plt\n",
    "        plt.title(classifiers_names[i] + ' ' + 'Receiver Operating Characteristic')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "        i += 1\n",
    "\n",
    "        # method II: ggplot\n",
    "        \n",
    "        df = pd.DataFrame(dict(fpr = fpr, tpr = tpr))\n",
    "        ggplot(df, aes(x = 'fpr', y = 'tpr')) + geom_line() + geom_abline(linetype = 'dashed')\n",
    "    \n",
    "    return roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score = calculate_roc_and_auc_2(delinquincy_test, outcomes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_roc_auc_score = pd.DataFrame({'classifiers':classifiers, 'roc_auc_score':roc_auc_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recommendation to someone working on the credit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
